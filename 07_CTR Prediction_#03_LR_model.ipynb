{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "a demo of LR-based Avazu-CTR prediction.\n",
    "    \n",
    "    1.apply one-hot transform to dataset.\n",
    "    2.using SGDClassifier for the training of Logistic Regression.\n",
    "    3.predicting to generate submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/05/02 00:33:24\n",
      "1.16.2\n",
      "3.0.3\n",
      "0.24.2\n",
      "0.20.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pandas import Series,DataFrame\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "import sklearn\n",
    "\n",
    "from datetime import datetime\n",
    "print(datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 8\n",
    "print(np.version.full_version)\n",
    "print(matplotlib.__version__)\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_train = \"./avazu-ctr-prediction/train.csv\"\n",
    "\n",
    "fp_train_col_counts = \"./avazu-ctr-prediction/col_counts\"\n",
    "\n",
    "fp_train_f = \"./avazu-ctr-prediction/train_f.csv\"\n",
    "\n",
    "fp_train_lb = \"./avazu-ctr-prediction/train_f_lb\"\n",
    "fp_train_oh = \"./avazu-ctr-prediction/train_f_oh\"\n",
    "\n",
    "fp_test = \"./avazu-ctr-prediction/test.csv\"\n",
    "fp_test_f = \"./avazu-ctr-prediction/test_f.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mask =  ['C1','banner_pos', \n",
    "                  'site_id', 'site_category', \n",
    "                  'app_domain', 'app_category', \n",
    "                  'device_model', 'device_type', 'device_conn_type', \n",
    "                  'C15', 'C16',  \n",
    "                  'C18', 'C19', 'C20', 'C21']\n",
    "\n",
    "#print(set(features_mask_org) - set(features_mask))\n",
    "#features_mask = features_mask + ['day_week', 'hour_day']\n",
    "\n",
    "features_unique = 'id'\n",
    "\n",
    "target_mask = 'click'\n",
    "\n",
    "## data reading\n",
    "#date_parser = lambda x: pd.datetime.strptime(x, '%y%m%d%H')\n",
    "def rawcount(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    lines = 0\n",
    "    buf_size = 1024 * 1024\n",
    "    read_f = f.raw.read\n",
    "\n",
    "    buf = read_f(buf_size)\n",
    "    while buf:\n",
    "        lines += buf.count(b'\\n')\n",
    "        buf = read_f(buf_size)\n",
    "\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # to store temporary variable\n",
    "oh_enc = pickle.load(open(fp_train_oh, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f = pd.read_csv(fp_train_f, chunksize = 500000, iterator = True,\n",
    "                           dtype={'id':str}, index_col=None\n",
    "                           #parse_dates=['hour'],\n",
    "                           #date_parser=date_parser\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier  # using SGDClassifier for training incrementally\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "lr_model = SGDClassifier(loss='log')  # using log-loss for LogisticRegression\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "～500000 : 2019/05/02 00:33:59\n",
      "～1000000 : 2019/05/02 00:38:44\n",
      "～1500000 : 2019/05/02 00:43:24\n",
      "～2000000 : 2019/05/02 00:47:52\n",
      "～2500000 : 2019/05/02 00:52:20\n",
      "～3000000 : 2019/05/02 00:56:46\n",
      "～3500000 : 2019/05/02 01:01:24\n",
      "～4000000 : 2019/05/02 01:56:34\n",
      "～4500000 : 2019/05/02 02:01:01\n",
      "～5000000 : 2019/05/02 02:05:25\n",
      "～5500000 : 2019/05/02 02:09:49\n",
      "～6000000 : 2019/05/02 02:14:23\n",
      "～6500000 : 2019/05/02 07:24:30\n",
      "～7000000 : 2019/05/02 07:29:18\n",
      "～7500000 : 2019/05/02 07:33:43\n",
      "～8000000 : 2019/05/02 07:38:13\n",
      "～8500000 : 2019/05/02 07:43:45\n",
      "～9000000 : 2019/05/02 07:49:54\n",
      "～9500000 : 2019/05/02 07:56:15\n",
      "～10000000 : 2019/05/02 08:02:46\n",
      "～10500000 : 2019/05/02 08:08:19\n",
      "～11000000 : 2019/05/02 08:14:12\n",
      "～11500000 : 2019/05/02 08:19:57\n",
      "～12000000 : 2019/05/02 08:25:27\n",
      "～12500000 : 2019/05/02 08:31:32\n",
      "～13000000 : 2019/05/02 08:37:26\n",
      "～13500000 : 2019/05/02 08:42:49\n",
      "～14000000 : 2019/05/02 08:47:52\n",
      "～14500000 : 2019/05/02 08:52:33\n",
      "～15000000 : 2019/05/02 08:57:16\n",
      "～15500000 : 2019/05/02 09:01:44\n",
      "～16000000 : 2019/05/02 09:06:27\n",
      "～16500000 : 2019/05/02 09:10:57\n",
      "～17000000 : 2019/05/02 09:15:44\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "sum = 0\n",
    "for chunk in df_train_f:\n",
    "    \n",
    "    sum +=  len(chunk.index)\n",
    "    print(\"～{} : {}\".format(sum, datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")))\n",
    "    \n",
    "    df_train = oh_enc.transform(chunk)\n",
    "    \n",
    "    del chunk\n",
    "    \n",
    "    train_X = df_train[df_train.columns.drop(['id', 'click'])]\n",
    "    train_y = df_train['click'].astype('int')\n",
    "    lr_model.partial_fit(train_X, train_y, classes = [0,1])  # fitting   \n",
    "    \n",
    "    # the score of training\n",
    "    y_pred = lr_model.predict_proba(train_X)[:, 1]\n",
    "    score = log_loss(train_y, y_pred)\n",
    "    \n",
    "    del train_X\n",
    "    del train_y\n",
    "    del y_pred\n",
    "    del df_train\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "print(\"end at {}\".format(datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")))\n",
    "\n",
    "del df_train_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lr-model\n",
    "fp_lr_model = \"./avazu-ctr-prediction/lr_model\"\n",
    "\n",
    "import pickle  # to store temporary variable\n",
    "## store the pre-trained lr_model\n",
    "pickle.dump(lr_model, open(fp_lr_model, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the training curve\n",
    "f1 = plt.figure(1)\n",
    "plt.plot(scores)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('log_loss')\n",
    "plt.title('log_loss of training')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
